# 

OSI 7层模型和TCP/IP 5层模型

**OSI 7层模型（理论标准）**

- 应用层
- 表示层
- 会话层
- 传输层
- 网络层
- 数据链路层
- 物理层



**TCP/IP 5层模型（事实标准）**

- 应用层（http/https, DNS）
- 传输层（TCP/UDP）
- 网络层（IP层）
- 数据链路层
- 物理层

*应用层和传输层会对外暴露编程接口Socket*



# 来源

[UID：239786645](https://space.bilibili.com/239786645)



# HTTP协议

http主要有1.0、1.1、2三个版本，之上还有https

# http 介绍

## 报文结构

起始行 + Header + Body

1. 起始行：标记这是个`http request`或者是`http response`
2. Header：KV结构，可以自定义存储多个KV
3. Body：一个字符串

> Get类型请求下，Body为空

> 对于网页，response body就是一个html串，浏览器解析后显示。

## 特征

1. 内容是明文的；文本协议，不是2进制协议，人可以直接查看协议内容
2. “一来一回”：客户端发起一个TCP连接，在连接上面发一个HTTP Request请求到服务器，服务器返回一个HTTP Response，然后连接关闭，每来一个请求，就要开一个连接，请求完了，连接关闭。

## http的两个典型问题

1. 性能问题：连接的建立和关闭都是耗时操作。
2. 服务器推送问题：不支持“一来多回”，服务器无法在客户端没有请求的情况下主动向客户端推送消息。

# 解决重开连接的性能问题：Keey Alive机制

1. 客户端在HTTP请求的头部加一个字段`Connection: Keep-Alive`。服务器受到带有这样的字段的请求，在处理完之后不会关闭连接，同时在HTTP Response里面也会加上这个字段，然后等待客户机在该连接上发送下一个请求。
2. 但是这样会带来一个问题：连接数有限。因此，服务器会有一个`Keep-Alive timeout`参数，一段时间后，如果该连接上没有新的请求进来，就会关闭连接。
3. 连接复用之后又产生了一个新的问题：即使一个请求处理完了，连接也不关闭，那么客户端怎么知道这个HTTP请求结束了呢？客户端怎么知道接收到的数据包是完整的呢？答案是在HTTP response头部返回了一个`Content-Length: xxx` 字段，这个字段告诉`HTTP Response`的Body共有多少个字节，客户端接收到这么多个字节以后，就知道响应成功接收完毕。

# http 1.1 特性

**KeepAlive变成了缺省机制**

除非加上Connection: Close字段属性，服务器才会在处理完之后关闭。

**引入Chunk机制**

响应的头部加上`Transfer-Encoding: chunked`属性，其目的是告诉客户端，相应Body是分成了一块块的，块与块之间有间隔符，所有块的结尾也有个特殊标记。这样，即使没有Content-Length字段，也能**方便客户端判断出响应的末尾**。

**Pipeline机制**

为了解决同一个连接上请求始终串行的问题，引入Pipeline机制，即：同一个TCP连接上面，可以在一个请求发出去之后，响应没有回来之前，就可以发送下一个、再下一个请求，这样提高了同一个TCP连接的响应效率。

但是，**pipeline机制存在阻塞问题**，虽然服务端可以按照接收的顺序进行返回，但是客户端同样要按照请求的顺序进行接收！

>  HTTP Pipelining并不能彻底解决Head ofline blocking问题。

![image-20220327150805366](https://hong-not-pic-1258424340.cos.ap-nanjing.myqcloud.com/notepic/202203271508511.png)



# http 1.1 的性能优化技术

**Spriting技术：**

针对小图片，服务器里把小图片拼成一张大图，到了浏览器再通过js或者css从大图中截取。

**内联Inlining：**

内联是另外一种针对小图片的技术，它将图片的原始数据嵌入在CSS文件里面如下所示：

```css
.icon1 {
    background: url(data:image/png;base64, <data>) no-repeat;
}
```

**js拼接：**

把大量小的js文件合并成一个大的，再下载

**请求的分片技术：**

将资源放在其他域名。

# 如何实现一来多回？

1. 客户端定期轮询
2. FlashSocket/WebSocket，不再基于HTTP，而是基于TCP，有局限性
3. HTTP长轮询：客户端发送一个HTTP请求，如果服务器有新消息，就立刻返回，没有就夯住此连接，客户端一直等待返回。
4. HTTP Streaming发送一个没完没了的Chunk流，只有一个连接，但是永远接收不完。

# 断点续传（下载）的原理

重新连接后，在请求的头部加上Range: first offset ~ last offset，指定从某个offset下载某个offset

# Http/2的特性

引入**SPDY**（读作Speedy）协议

兼容Http1.1，所以在设计上和http1.1不在一个层级。

没有改变url的格式（为了兼容http 1.1）

没有改变Http request、response报文结构（为了兼容）

## SPDY协议的原理和特征

**原理：**

在SSL层上增加一个SPDY会话层，以在一个TCP连接中实现并发流。

通常的HTTP GET和POST格式仍然是一样的；然而SPDY为编码和传输数据设计了一个新的帧格式。

流是双向的，可以在客户端和服务器端启动。

SPDY旨在通过基本（始终启用）和高级（可选启用）功能实现更低的延迟。 

特征：

- **复用流：**SPDY允许在一个连接上无限制并发流。因为请求在一个通道上，TCP效率更高：更少的网络连接，发出更少更密集的数据包。
- **请求优先级：**虽然无限的并发流解决了序列化的问题，但他们引入了另一个问题：如果带宽通道受限制， 客户端可能会因防止堵塞通道而阻止请求。为了克服这个问题，SPDY实行请求优先级：客户端从服务器端请求它希望的项目数量，并为每个请求分配一个优先级。这可以防止在网络通道被非关键资源堵塞时，高优先级的请求被挂起。
- **HTTP报头压缩：**SPDY压缩请求和响应HTTP报头，从而减少传输的数据包数量和字节数。

# http/2的二进制分帧：尝试解决1.1队头阻塞问题

将传输的数据分成若干个小的帧包，从而降低阻塞的概率。

这样会导致顺序请求会乱序返回。

>  但是没有彻底解决，因为TCP本身要求先进先出！



# 加密

## 对称加密的概念

客户端和服务端使用同一个密钥。

1. 密钥A的传输需要另一个密钥B，B又需要C，则成了连环套问题
2. 密钥的存储是困难的。

## 双向非对称加密的概念

双方各有一个私钥和一个公钥

公钥是私钥算出来的，但是反向不行

公钥是公开的，不需要保密。

**如何操作：**

1. 私钥签名，公钥验签，目的是为了防篡改、防抵赖。
2. 公钥加密，私钥解密，防止第三方拦截和偷听。

## 单向非对称加密（互联网的通信场景）

在互联网上，网站是对外公开的。

客户端加密发送给服务器，服务器给客户端发送明文。

常见的场景是，客户端向服务端发送自己的公钥，服务端使用得到的公钥通信。

**中间人攻击问题 - 劫持公钥篡改转发：**

问题的根源是公钥是明文传输的。

服务端、客户端没办法验证对方的合法性。

1. 服务端不再发公钥，而是发数字证书，反过来也一样，数字证书是公钥的加强版。
2. 数字证书是CA机构颁发。



# 证书和Certification Authority

> 证书是用来证明网站合法性的证明，CA则是颁发证书的签发机构

## 证书是如何颁发？如何验证？

服务端将个人信息和公钥发送给CA，CA获取到后用私钥签名生成证书。

用户访问服务端的时候会下载证书，然后到CA进行验证。

而CA的合法性是由上级CA授予的，上级又是由更上级授予的。

**CA信任链，最终指向 Root CA**

用户的操作系统、浏览器发布的时候，已经嵌入了证书，当你信任电脑、信任浏览器，就意味着你信任Root CA。

最终，证书成为网络上每个通信实体的身份证，在网络上传输的都是证书，而不再是原始的那个公钥。把这套体系标准化后，就是网络安全领域常见的**public key infrastructure**即**公钥基础设施**。

# SSL/TLS协议

Secure Sockets Layer

Transport Layer Security 

安全套接层，传输层安全协议

https = http + ssl/tls

它在传输层和应用层之间。

# Https

https协议的本质是：TCP+SSL/TLS+http

核心是增加了**Secure Sockets Layer**、**Transport Layer Security**的使用。

相对其他HTTP协议，主要是在安全上做出了改进。

![image-20220327155125930](https://hong-not-pic-1258424340.cos.ap-nanjing.myqcloud.com/notepic/202203271551554.png)

单向https：单向非对称加密。只客户端验证服务器的合法性，大多数网站都是如此。

双向https：双向非对称加密。客户端验证服务器合法性服务器也要验证和护短的合法性。



## https和http2的关系

http2主要是解决性能问题

https解决的是安全问题。

理论上二者没有必然相关。

但在实践层面，目前主流的浏览器要求如果要支持http2则必须先支持https。

整个互联网都在推动https的普及。



# DNS

# 公网DNS和内网DNS服务器的使用场景

公网：用户通过公网访问网站

内网：屏蔽外网IP

## 公网DNS 的解析

电信LocalDNS 服务器

根域名服务器

com域名服务器（2级域名授权DNS）

GSLB（3级、4级授权DNS）

GSLB：广域网负载均衡，大厂自己研发的DNS服务器

一个域名对应多个ip，网站在全国多地部署，用户就近访问。

因为通过了电信DNS服务器，所以可能存在dns劫持问题。

**使用httpDNS，可以直接绕过电信Dns服务器，防劫持**

域名服务器本身没有域名



**IP只有一个，如何绑定到多个服务器上去？**

BGP anycast技术！



# Transmission Control Protocol

# 如何实现可靠传输

> 网络传输的本质是不可靠的

可靠传输具有三重语义：

1. 数据包不丢
2. 数据包不重
3. 顺序不乱



如何不丢？重发！

> 如果客户端在超时时间内没有受到ACK，则重发数据。
>
> 客户端对发送的每个数据包编一个号，递增，给予编号能够进行顺序的确认。
>
> 若ACK=3，表达的是小于等于3的数据包都已经收到了。



为什么会重复？

> 因为只要超过了约定时间，客户端还没有收到服务器的确认，就会重发。
>
> 但是也许ACK确认信息已经在网络上了，只是还没有到达。所以要进行判重。



如何判重？

> 顺序ACK。
>
> 服务器给客户端回复ACK=6，意思是所有小于等于6的数据包全部收到了，之后凡是收到这个范围内的数据包，判定为重复的包，服务器收到后丢弃即可。



> TCP这种思想可以说是朴素而深刻的，分布式系统中消息中间件的消息不重、不漏也是借鉴了这个思想。



# 连接的本质是什么

在物理层面，或许数据包1、2、3走的是不同的网络链路。

但是事实上，在客户端和服务器之间没有存在这么一条“物理”链路。

每一条连接的是一个**状态机**。

每条连接用（客户端IP、客户端Port、服务器IP、服务器Port）4元组唯一确定，在代码中就是一个个的Socket。

每个连接会有五个字段，除了4元组，还有一个State，State通常有11种状态。

11种状态，有些事客户端、服务器都有的，有些是只存在其中一方的。

![image-20220327173557640](https://hong-not-pic-1258424340.cos.ap-nanjing.myqcloud.com/notepic/202203271735605.png)

1. 首先，客户端和服务器都是CLOSED状态；
2. 连接建立好之后，双方都处于ESTABLISHED状态，开始传输数据；
3. 最后连接关闭，双方会再次回到CLOSED状态。为什么开始是处于CLOSED状态，而没有一个INIT状态呢？因为状态是服用的，每个连接用4元组唯一标识，关闭之后，后面又会开启，所以没有必要引入一个“初始”状态。





# TCP的三次握手和四次挥手

> ACK（Acknowledgement Number） ：确认编号
>
> SYN（Synchronize Sequence Number）：同步序列编号
>
> RST（Reset）：复位标志
>
> URG（The Urgent Pointer）：紧急标志
>
> PSH（Push）：推标志
>
> FIN（Finish）：结束标志



TCP协议是7层网络协议中的传输层协议，负责数据的可靠传输。在建立TCP连接时，需要通过三次握手来建立，过程是:

1. 客户端向服务端发送一个SYN
2. 服务端接收到SYN后，给客户端发送一个SYN_ACK
3. 客户端接收到SYN_ACK后，再给服务端发送一个ACK



客户端状态转移的过程是：CLOSED -> SYN_SENT -> ESTABLISHED

服务器的状态转移过程是：CLOSED -> LISTEN -> SYN_RCVD -> ESTABLISHED

![image-20220328144921168](https://hong-not-pic-1258424340.cos.ap-nanjing.myqcloud.com/notepic/202203281449426.png)





在断开TCP连接时，需要通过四次挥手来断开，过程是:

1. 客户端向服务端发送FIN

2. 服务端接收FIN后，向客户端发送ACK，表示我接收到了断开连接的请求，客户端你可以不发数据了，不过服务端这边可能还

3. 服务端处理完所有数据后，向客户端发送FIN，表示服务端现在可以断开连接

4. 客户端收到服务端的FIN，向服务端发送ACK，表示客户端也会断开连接了



为什么是4次？因为tcp是全双工的，可以处于Half-close状态

![image-20220328150240227](https://hong-not-pic-1258424340.cos.ap-nanjing.myqcloud.com/notepic/202203281502780.png)



为什么要又TIME_WAIT状态，而不是直接Close？

因为双方都进入Close之后，人可能会有数据包还在网络上闲逛，4元组无法区别新老连接，那么会导致之前的网络数据包会在新连接打开后被当作新数据包！

如何解决呢？

在TCP上定义了一个值叫做MSL（Maximum Segment Lifetime），任何一个IP数据包在网络上逗留的最长时间是MSL，默认是120s。

如果超出了这个时间，中间路由节点就会把该数据包丢弃。

客户端TIME_WAIT会等待2倍MSL再进入Close。

这是因为客户端第四次数据包发送的时间和服务端第三次数据包的时间MSL加起来。

服务端之所以没有TIME_WAIT状态是因为当客户端处于这个状态时要等待2倍MSL，服务端想使用连接也是用不了的。



为了防止大量连接处于TIME_WAIT从而耗光资源，通常采取如下措施

1. 不要让服务器关闭连接
2. 客户端做连接池，复用连接，不要频繁的创建和关闭。这也是http1.1和http/2采用的思路。



# 粘包/分包、数据分段

**粘包：**

客户端write两次，2个数据包，假设是“abc”，“def”

服务器read一次，读取的可能是“abcdef”，也可能是“abcd”，然后ef在下次被读取到。

**分包：**

客户端write一次，1个数据包，假设是“abcdef”

服务器read两次才读完，读取的可能是“ab”，“cdef”

**原因：**TCP是一个面向流的协议。

**如何解决？**

> TCP协议不负责解决这个问题，要程序员自己解决

**方法一：每个数据包前面加一个4字节长度的字段**

**方法二：每个数据包前后，加一个固定长度的特殊标记**

不如方法一简洁，可能遇到数据内容和特殊标记一样了。



TCP的MSS（Maximum Segment Size）

TCP自己不会做数据包分段，而是到了IP层，IP层发现数据包大于MTU，就会IP分片，把一个包拆成多个包。

效率问题、分片会增大丢包的概率

为了避免，可以双方商量MSS值，可以在大多数情况下生效，但是有些局域网的MTU不是1500，或者小于1500，那么依然还是会分片的。

# TCP流量控制、滑动窗口

![image-20220328203418695](https://hong-not-pic-1258424340.cos.ap-nanjing.myqcloud.com/notepic/202203282034850.png)

![image-20220328203546259](https://hong-not-pic-1258424340.cos.ap-nanjing.myqcloud.com/notepic/202203282035307.png)

非阻塞的编程模式：应用程序是和发送、接收缓冲区打交道，而不是直接调用tcp/ip协议栈。

1. 应用程序调用write的时候，数据只是写入了本机的发送缓冲区里面，并没有立即发送到网络上。
2. 应用程序调用read的时候，并不会等待网络上的数据的到来，只是读取本地接收缓冲区的数据，读不到就自动返回了。

发送接收缓冲区的知识点：

1. 无论客户端还是服务端，每条TCP连接上，都有一个发送缓冲区和一个接收缓冲区。所以TCP连接的数越多，耗的内存越多。

2. 查看缓冲区大小的方法

   ```c
   net.ipv4.tcp_wmem = 4096, 16384, 4194304
   net.ipv4.tcp_rmem = ...
   三个值分别是min(4k)、default(16k)、max
   ```

3. 编程可以设置发送、接收缓冲区的大小。操作系统也可能根据内存使用情况动态的调整缓冲区的大小。



![image-20220329142527570](https://hong-not-pic-1258424340.cos.ap-nanjing.myqcloud.com/notepic/202203291425542.png)



窗口收缩到0，可能引起的“死锁”问题

发送窗口收缩到0之后，发送方将不能再继续和接收方通信，要等待接收方新的ACK到来，确定新的发送窗口大小。

如果新的ACK丢失，发送方一直等待接收方扩大窗口，接收方一直等待发送方发新数据，可能造成“死锁”，互等待。解决办法:

发送窗口收缩到0之后，发送方启动一个定时器，定期给接收方发送探测报文，看窗口是否扩大。多次探测，窗口仍然还是0，就可能直接关闭连接了。



**提高网络吞吐率的基本思想**

> IP网络的设计初衷就要提高网络的吞吐率，提高网络传输效率，所以不希望有大量小包在网络上传输，浪费网络带宽

小包：<MSS的包

极小包： header ( TCP头＋IP头）> data size

为了尽力避免小包，采取“批量策略”:

1.发送方延迟发送，累积一批一次性发送-Nagle算法

2.接收方延迟ACK，累积一批一次性ACK



**Nagle算法：延迟发送策略**

> 默认情况下，Nagle算法是开启的。要禁用Nagle，设置TCP的属性TCP_NODELAY

Nagle算法的核心原则:任意时刻，只允许有一个未被ACK的小包存在。Nagle算法的几条规则:

（1）如果包长度>= MSS，允许发送

（2）包含有FIN，允许发送

（3）设置了TCP_NODELAY，允许发送

（4）发生超时(一般200ms) ，允许发送

不满足这几个条件的，数据就会呆在发送缓冲区里面，暂时不发送。

Nagle算法可能引起网络延迟问题（它是牺牲延迟，换取高吞吐），所以对延迟非常敏感的应用，一般都会禁用Nagle算法。

禁用Nagle算法之后，上层应用根据业务场景自己去控制数据的发送时机，从而在“延迟”和“吞吐”上取得平衡。



**延迟ACK**

第一种场景：

![image-20220329143054163](https://hong-not-pic-1258424340.cos.ap-nanjing.myqcloud.com/notepic/202203291430157.png)

第二种场景：

没有立即ACK，而是等一会，刚好有新的包发送，就一起发。

![image-20220329143139889](https://hong-not-pic-1258424340.cos.ap-nanjing.myqcloud.com/notepic/202203291431396.png)

**延迟多久呢?**

一般是200ms。接收方有个定时器，周期性每200ms看一下，是否有ACK需要发出去，如果有，就发出去。

所以延迟时间其实是<= 200ms，假设收到数据是在第160ms，则40ms之后就会进行ACK，而不是200ms进行。

**基本原理总结:”时延“和”吞吐率"的折中与平衡**

无论是延迟发送，还是延迟ACK，都是在”时延“和”吞吐率“之间来回平衡:

现实世界中“公交车”的例子，2种策略:1.到点就发车:

时延低（用户等待时间短)，但吞吐率低（车坐不满）﹔2.等人满了再发车:

时延高（用户等待时间长)，但吞吐率高

但有个反例:假设乘公交的人一直很多，人满了就发车，时延最低，吞吐率也最大。

# TCP的拥塞机制

**流量控制：**

点对点的问题，只牵扯发送、接收方；

解决的是接收方处理不过来的问题。

对应场景：中间好、两头差（接收方能力差）

**拥塞控制：**

全局问题，牵扯整个网络

解决的是中间节点（路由器、交换机）处理不过来的问题。

对应场景：两头好、中间差（网络差）



发送窗口 = min（拥塞窗口， 接收窗口）

拥塞窗口是动态调节的，只要ACK不超时，就会先指数级增长，然后再线性增长，当发生拥塞，又快速下降。



# 发送方超时重传的超时时间设置多少合适？

> 设置太短，很容易超时，不该重传的也重传了。浪费宽带资源
>
> 设置太长：时延太长了，很长时间才会重传。
>
> 不能是写死的值。



最朴素的想法是，**超时时间根据RTT来算。**超时时间稍微大于RTT。

这是个典型的**时间序列预测算法**！！——机器学习的味道来了。

$ RTT_{smooth(新)} = (1-a) * RTT_{smooth(老)} + a*RTT_{(最新值)} $

*a是常数，推荐0.125*



这其实就是时间序列预测中的典型**EWMA算法（指数加权移动平均）**

有了$RTT_s$，RTO（Retransmission TIme Out  超时重传的超时时间）就简单了

$RTO = RTT_s + 4 * RTT_d$

*$RTT_s$ 刚算出来的预测值*

*$RTT_d$ 每次预测值和实际值的偏差*



**快重传**

快重传和延迟ACK是矛盾的。

要时间快重传，需要两点：

1. 接收方一旦收到失序的报文，马上ACK，不要延迟ACK。
2. 发送方一连收到3个重复的ACK，马上重传还未收到的报文。



**快恢复**

没有快恢复时，窗口增长曲线是指数增长、线性增长、掉落、往复。

快恢复则是：

![image-20220328160446519](https://hong-not-pic-1258424340.cos.ap-nanjing.myqcloud.com/notepic/202203281604708.png)



**快恢复和快重传是搭配使用的。**

> 拥塞控制的这些策略，只是在尽力降低拥塞发生的概率，而不能完全解决拥塞，毕竟发送方对整个网络的信息量是有限的。
>
> 所以策略是自适应的，基于历史数据预测+ACK反馈快慢来调整。这个调整可能有效，也可能无效。

